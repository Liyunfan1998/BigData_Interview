{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# -*- coding: UTF-8 -*-\n",
    "fd = open('ToBeCleaned.txt','r')\n",
    "vec=fd.readlines()\n",
    "for a in vec:\n",
    "    c = a.split('日')[0]\n",
    "    l = len(c)\n",
    "    c = c[0:l-1]\n",
    "    if c.endswith('1'):\n",
    "        l = len(c)\n",
    "        c = c[0:l-1]\n",
    "    c = c.replace('+',',')\n",
    "    a = c\n",
    "    print(c)\n",
    "numpy.savetxt('new.csv', vec, delimiter = ',')  \n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sqlite\n",
    "import os, sys\n",
    "import jieba, codecs, math\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg\n",
    "f = codecs.open('lastLine.txt', 'r', 'utf-8')\n",
    "content = f.read().strip()  # 获取内容\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jieba import analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = analyse.extract_tags\n",
    "# 使用自定义停用词集合\n",
    "analyse.set_stop_words(\"stopwords.bat\");\n",
    "keywords = tfidf(content,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw=[]\n",
    "# 输出抽取出的关键词\n",
    "for keyword in keywords:\n",
    "    kw.append(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8           \n",
    "import os    \n",
    "import sys  \n",
    "import numpy as np  \n",
    "import matplotlib  \n",
    "import scipy  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn import feature_extraction    \n",
    "from sklearn.feature_extraction.text import TfidfTransformer    \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import HashingVectorizer   \n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "  \n",
    "  \n",
    "    #存储读取语料 一行预料为一个文档   \n",
    "    corpus = []  \n",
    "    for line in open('lastLine.txt', 'r').readlines():  \n",
    "        #print line  \n",
    "        corpus.append(line.strip())  \n",
    "    #print corpus  \n",
    "      \n",
    "    #将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频  \n",
    "    vectorizer = CountVectorizer()  \n",
    "    print (vectorizer ) \n",
    "  \n",
    "    X = vectorizer.fit_transform(corpus)  \n",
    "    analyze = vectorizer.build_analyzer()  \n",
    "    weight = X.toarray()  \n",
    "  \n",
    "    print (len(weight)  )\n",
    "    print (weight[:5, :5])  \n",
    "  \n",
    "    #LDA算法  \n",
    "    print ('LDA:' ) \n",
    "    import numpy as np  \n",
    "    import lda  \n",
    "    import lda.datasets  \n",
    "    model = lda.LDA(n_topics=2, n_iter=500, random_state=1)  \n",
    "    model.fit(np.asarray(weight))     # model.fit_transform(X) is also available  \n",
    "    topic_word = model.topic_word_    # model.components_ also works  \n",
    "  \n",
    "    #文档-主题（Document-Topic）分布  \n",
    "    doc_topic = model.doc_topic_  \n",
    "    print(\"type(doc_topic): {}\".format(type(doc_topic)))  \n",
    "    print(\"shape: {}\".format(doc_topic.shape))  \n",
    "  \n",
    "    #输出前10篇文章最可能的Topic  \n",
    "    label = []        \n",
    "    for n in range(10):  \n",
    "        topic_most_pr = doc_topic[n].argmax()  \n",
    "        label.append(topic_most_pr)  \n",
    "        print(\"doc: {} topic: {}\".format(n, topic_most_pr))  \n",
    "          \n",
    "    #计算文档主题分布图  \n",
    "    import matplotlib.pyplot as plt    \n",
    "    f, ax= plt.subplots(6, 1, figsize=(8, 8), sharex=True)    \n",
    "    for i, k in enumerate([0, 1, 2, 3, 8, 9]):    \n",
    "        ax[i].stem(doc_topic[k,:], linefmt='r-',    \n",
    "                   markerfmt='ro', basefmt='w-')    \n",
    "        ax[i].set_xlim(-1, 2)     #x坐标下标  \n",
    "        ax[i].set_ylim(0, 1.2)    #y坐标下标  \n",
    "        ax[i].set_ylabel(\"Prob\")    \n",
    "        ax[i].set_title(\"Document {}\".format(k))    \n",
    "    ax[5].set_xlabel(\"Topic\")  \n",
    "    plt.tight_layout()  \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算TFIDF  \n",
    "corpus = []  \n",
    "  \n",
    "#读取预料 一行预料为一个文档   \n",
    "for line in open('lastLine.txt', 'r').readlines():  \n",
    "    #print line  \n",
    "    corpus.append(line.strip())  \n",
    "#print corpus  \n",
    "  \n",
    "#将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频  \n",
    "vectorizer = CountVectorizer()  \n",
    "  \n",
    "#该类会统计每个词语的tf-idf权值  \n",
    "transformer = TfidfTransformer()  \n",
    "  \n",
    "#第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵  \n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  \n",
    "  \n",
    "#获取词袋模型中的所有词语    \n",
    "word = vectorizer.get_feature_names()  \n",
    "  \n",
    "#将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重  \n",
    "weight = tfidf.toarray()  \n",
    "  \n",
    "#打印特征向量文本内容  \n",
    "# print ('Features length: ' + str(len(word))  )\n",
    "# for j in range(len(word)):  \n",
    "#     print( word[j]  )\n",
    "  \n",
    "#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重    \n",
    "wordFreq = 0\n",
    "for j in range(len(word)):\n",
    "    print( word[j]) \n",
    "    for i in range(len(weight)):\n",
    "        wordFreq+=weight[i][j];\n",
    "    print (weight[i][j]) \n",
    "    print( '\\n'  )\n",
    "print( '\\n'  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
